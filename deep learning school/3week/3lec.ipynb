{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ищем наше решение в виде последовательных логических утверждений - это логический алгоритм классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в листе содержится ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во время фита мы строим решающие правила для каждой вершины так, чтобы в вершине преобладал один класс над другим.\n",
    "\n",
    "В каждой вершине решающее правило имеет вид признак >= Const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Решающее дерево строится за счет минимизации выбранного критерия после каждого разделения\n",
    "- Глубина не должна быть слишком большой, чтобы не было оверфита\n",
    "- Если в лист попал 1 объект - это плохо, хоть это и даст оптимальное значение критерия - это не репрезентативно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У алогритма решающего дерева возможна регуляризация как раз путем ограничения глубины и количества объектов в терминальной вершине"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для регрессии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В листе будет находиться действительное число, а не метка класса\n",
    "\n",
    "В качестве лосса обычно MSE используют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Композиции алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Метод голосования:\n",
    "есть $a_1,...,a_n$ - обученных алгоритмов, тогда для объекта $x$:\n",
    "- Классификация: устраиваем голосование и относим $x$ туда, за который больше всего алгоритмов проголосовало\n",
    "- Регрессия: среднее значение $a_1(x),...,a_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо делить начальную выборку на бутстрепные выборки и потом каждый алгоритм обучать на отдельной бутстрепной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистика показывает, что путем создания выборок бустрепом - все они аналогичны исходной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/vqF8wrWjR5s?t=650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это как раз и есть bootstap + aggregation - обучаем n алгоритмов на n бутстрепных выборок и потом устраиваем голосование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бэггинг над решающими деревьями - это Случайный лес, это супер популярно на kaggle.com.\n",
    "\n",
    "Решающие деревья компенсируют свои недостатки в случае бэггинга, так как они умеют восстанавливать довольно сложные зависимости, а благодаря бэггингу случайный лес не так склонен к переобучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минусы:\n",
    "- Слишком долгие вычисления (но можно распараллеливать)\n",
    "\n",
    "- В индустрии стараются обойтись без композиций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Обучаем какой-то алгоритм - полученный столбец предсказаний воспринимаем как новый признак, так обучаем несколько алгоритмов и получаем\n",
    "новый датасет(или добавляем новые признаки к старому датасету), построенный из признаков равных предсказаниям разных алгоритмов.\n",
    "\n",
    "2)Обучаем какой-то супер крутой метаалгоритм на полученном датасете (а можно и на старом датасете вместе с новыми признаками) и уже получаем итоговый столбец предсказаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стэкинг круто использовать как финальный рывок в соревнованиях, потому что он очень тяжеловесный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "принимаем решение взвешенным голосованием - то есть каждый следующий алгоритм в цепочке компенсирует ошибки предыдущих"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь регрессия и классификация рассматривается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача - построить алогоритм: функцию $a(x)$, которая оптимизирует loss: $Q(y, a)=\\dfrac{1}{l}\\sum_{i=1}^lL(y^i, a(x^i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример для MSE: $L(y, a(x))=(a(x)-y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в бустинге используют решающие деревья в качестве базового алгоритма, суть алгоритма: https://youtu.be/JElfEE1OrSU?t=566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cуть: мы строим алгоритм $a(x)=c_1a_1(x)+...+c_ka_k(x)$, можно взять $c_k = \\dfrac{1}{\\sqrt{k}}$, все $a_j$ пусть решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим $a_{k+1}$ просто оптимизируя наш лосс после того как $a_k$ алгоритм построили, то есть для $a_{k+1}$ просто в качестве таргета для $x^i$ объекта(лейбл для обучения алгоритма) берем: $-\\dfrac{\\partial L(y^i, a)}{\\partial a}\\vert_{a = a_1(x^i)+...+a_k(x^i)}$. То есть каждый $a_k$ решает свою задачу регрессии со своими таргетами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На словах: построили первое решающее дерево, где вектор таргетов - это просто исходный вектор $y$ и получили некоторые предсказания, потом\n",
    "строим следующее дерево таким образом, чтобы снизить значение loss общей функции, то есть строим следующее дерево, у которого вектор таргетов - это антиградиент функции потерь с координатами - значениями производной $-\\dfrac{\\partial L(y^i, a)}{\\partial a}\\vert_{a = a_1(x^i)+...+a_k(x^i)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так можно продолжать добавлять $a_k$ в решающий алогоритм и каждый $a_k$ будет решать свою задачу регрессии со своими таргетами, уменьшая лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плюсы и минусы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полную ошибку можно разделить на сдвиг(bias) и дисперсию, плюс бустинга в том, что он устраняет bias, а не накапливает его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг нельзя обучать параллельно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бустинга необходимо: число объектов >> числа признаков"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
