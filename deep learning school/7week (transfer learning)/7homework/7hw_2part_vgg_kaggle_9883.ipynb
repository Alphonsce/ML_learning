{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Домашнее задание 7.**\n\n## **в этом ноутбуке ЧАСТЬ 2 домашнего задания:**\n\n**Моя цель: >99.5 скора**\n\n- Сначала я попробую разобраться с аугментацией, потому что это особо не было покрыто на семинарах\n\n- Затем я попробую хорошо зафайнтюнить несколько сеток\n\n- Попробую организовать ансамбль из обученных сеток","metadata":{"id":"bXBCVgdBT1ha"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport PIL\nimport random\nimport os\nimport time\n\ntrain_on_gpu = torch.cuda.is_available()\ntrain_on_gpu","metadata":{"id":"_yRiTjJGlGt_","outputId":"8682f8f9-3633-49b2-8c72-2b99487c6306","execution":{"iopub.status.busy":"2022-04-14T19:14:53.863064Z","iopub.execute_input":"2022-04-14T19:14:53.863378Z","iopub.status.idle":"2022-04-14T19:14:55.335381Z","shell.execute_reply.started":"2022-04-14T19:14:53.863298Z","shell.execute_reply":"2022-04-14T19:14:55.334522Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    '''Делает наши результаты воспроизводимыми (вычисления могут немного больше времени занимать)'''\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)","metadata":{"id":"1hH1-Fyr6sIN","execution":{"iopub.status.busy":"2022-04-14T19:14:55.337209Z","iopub.execute_input":"2022-04-14T19:14:55.337664Z","iopub.status.idle":"2022-04-14T19:14:55.345419Z","shell.execute_reply.started":"2022-04-14T19:14:55.337626Z","shell.execute_reply":"2022-04-14T19:14:55.344652Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataset import ConcatDataset\nimport pickle\nfrom skimage import io\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms, models\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","metadata":{"id":"izdFWIFZlg8G","execution":{"iopub.status.busy":"2022-04-14T19:14:55.346870Z","iopub.execute_input":"2022-04-14T19:14:55.347385Z","iopub.status.idle":"2022-04-14T19:14:56.623596Z","shell.execute_reply.started":"2022-04-14T19:14:55.347288Z","shell.execute_reply":"2022-04-14T19:14:56.622908Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# разные режимы датасета \nDATA_MODES = ['train', 'val', 'test']       # сами объекты датасетов для train и val по своей структуре не будут отличаться, они оба размечены\n\n# все изображения будут масштабированы к размеру 224x224 px\nRESCALE_SIZE = 224\nDEVICE = torch.device(\"cuda\")","metadata":{"id":"2d4n0ZuMll3b","execution":{"iopub.status.busy":"2022-04-14T19:14:56.626530Z","iopub.execute_input":"2022-04-14T19:14:56.626925Z","iopub.status.idle":"2022-04-14T19:14:56.631871Z","shell.execute_reply.started":"2022-04-14T19:14:56.626887Z","shell.execute_reply":"2022-04-14T19:14:56.631221Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n    \"\"\"\n    Класс датасета с картинками, который паралельно подгружает их из папок\n    производит скалирование и превращение в торчевые тензоры\n\n    Особенность нашего случая в том, что метки классов содержатся в названиях картинок, а не в\n    отдельном цсв файле\n    \"\"\"\n    def __init__(self, files, mode, transform=None):\n        super().__init__()\n        # список файлов для загрузки\n        self.files = sorted(files)\n        # режим работы\n        self.mode = mode\n        self.transform = transform\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]     # извлекаем все возможные классы из названий картинок\n            self.label_encoder.fit(self.labels)     # лейбл энкодер фитим на этих названиях классов\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)     # функция, позволяющая записывать объект в файл\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        ''' По пути к картинке возвращает PIL Image '''\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index):\n        # Датасет у нас хранится в виде некоторых картинок, но как только мы достаем из него элемент с ним делается __getitem__\n        transform = self.transform\n        if transform is None:\n            transform = transforms.Compose([\n            transforms.ToTensor(),      # кстати totensor приводит итак все величины к от 0 до 1\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # видимо по каждому каналу уже были посчитаны\n            ])\n\n        x = self.load_sample(self.files[index])\n        x = self._prepare_sample(x)\n        # x = np.array(x / 255, dtype='float32')    # это вообще лишнее действие - оно в ToTensor делается, а с np.array плохо трансформации работают\n        x = transform(x)\n        if self.mode == 'test':\n            return x        # если тестовый датасет, то возвращаем чисто тензор нормализованный по каналам\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n\n            return x, y     # возвращает элемент датасета в виде нормализованного по каналам торч тензора и его метки\n        \n    def _prepare_sample(self, image):\n        ''' рескейлит PIL Image '''\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return image\n        # return image        # гораздо лучше PIL Image возвращать, потому что с ним все трансформации работают","metadata":{"id":"rJ2-DuTRlnn2","execution":{"iopub.status.busy":"2022-04-14T19:14:56.634932Z","iopub.execute_input":"2022-04-14T19:14:56.635158Z","iopub.status.idle":"2022-04-14T19:14:56.751541Z","shell.execute_reply.started":"2022-04-14T19:14:56.635132Z","shell.execute_reply":"2022-04-14T19:14:56.749698Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = Path('../input/journey-springfield/train/simpsons_dataset')\nTEST_DIR = Path('../input/journey-springfield/testset/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))    # рекурсивно собирает все файлы из указанной директории формата jpg\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","metadata":{"id":"CE6-MD7Tlpaj","execution":{"iopub.status.busy":"2022-04-14T19:14:56.753005Z","iopub.execute_input":"2022-04-14T19:14:56.753412Z","iopub.status.idle":"2022-04-14T19:15:02.267537Z","shell.execute_reply.started":"2022-04-14T19:14:56.753374Z","shell.execute_reply":"2022-04-14T19:15:02.266594Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\n\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.25, stratify=train_val_labels)\nn_classes = len(np.unique(train_val_labels))","metadata":{"id":"LyWd_RcamFMJ","execution":{"iopub.status.busy":"2022-04-14T19:15:02.269142Z","iopub.execute_input":"2022-04-14T19:15:02.269435Z","iopub.status.idle":"2022-04-14T19:15:02.388002Z","shell.execute_reply.started":"2022-04-14T19:15:02.269383Z","shell.execute_reply":"2022-04-14T19:15:02.387341Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"val_dataset = SimpsonsDataset(val_files, mode='val')\ntrain_dataset = SimpsonsDataset(train_files, mode='train')\ntest_dataset = SimpsonsDataset(test_files, mode=\"test\")\n\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","metadata":{"id":"L6vXKbq8mk-Q","execution":{"iopub.status.busy":"2022-04-14T19:15:02.390082Z","iopub.execute_input":"2022-04-14T19:15:02.390545Z","iopub.status.idle":"2022-04-14T19:15:02.598167Z","shell.execute_reply.started":"2022-04-14T19:15:02.390507Z","shell.execute_reply":"2022-04-14T19:15:02.597480Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, scheduler):\n    '''\n    Функция обучения по всем батчам 1 раз (1 эпоха)\n    '''\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n    iter = 1\n    start_time = time.time()\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n\n        # добавил, чтобы при запуске обучения видеть сколько примерно учиться\n        if iter == 5:\n            print(f'среднее время на одну итерацию: {round((time.time() - start_time) / 5, 4)}')\n        iter += 1\n\n    scheduler.step()\n            \n    train_loss = running_loss / processed_data\n    train_acc = running_corrects.cpu().numpy() / processed_data\n\n    return train_loss, train_acc","metadata":{"id":"-Kb1KVdumnA5","execution":{"iopub.status.busy":"2022-04-14T19:15:02.601039Z","iopub.execute_input":"2022-04-14T19:15:02.601238Z","iopub.status.idle":"2022-04-14T19:15:02.610788Z","shell.execute_reply.started":"2022-04-14T19:15:02.601214Z","shell.execute_reply":"2022-04-14T19:15:02.610117Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc","metadata":{"id":"4P_OBWZtmoi5","execution":{"iopub.status.busy":"2022-04-14T19:15:02.614097Z","iopub.execute_input":"2022-04-14T19:15:02.614442Z","iopub.status.idle":"2022-04-14T19:15:02.623257Z","shell.execute_reply.started":"2022-04-14T19:15:02.614414Z","shell.execute_reply":"2022-04-14T19:15:02.622367Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(train_data, val_data, model, optimizer, scheduler, epochs, batch_size, save_path='/kaggle/working/model.pth'):\n    '''\n    Полный цикл обучения\n    '''\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer, scheduler)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            if epoch != 0:\n                if val_acc > history[-1][-1]:      # Если качество на трейне и на валидации выросло, то сохраняем модельку\n                    torch.save(model.state_dict(), save_path)\n\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n            \n    return history","metadata":{"id":"3NJFLt5lmqF6","execution":{"iopub.status.busy":"2022-04-14T19:15:02.624485Z","iopub.execute_input":"2022-04-14T19:15:02.624832Z","iopub.status.idle":"2022-04-14T19:15:02.635570Z","shell.execute_reply.started":"2022-04-14T19:15:02.624754Z","shell.execute_reply":"2022-04-14T19:15:02.634759Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_loader):\n    ''' Возвращает предсказанные вероятности принадлежности к классу для переданного лоадера '''\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","metadata":{"id":"zWJn0A6_m-YO","execution":{"iopub.status.busy":"2022-04-14T19:15:02.637022Z","iopub.execute_input":"2022-04-14T19:15:02.637304Z","iopub.status.idle":"2022-04-14T19:15:02.646595Z","shell.execute_reply.started":"2022-04-14T19:15:02.637271Z","shell.execute_reply":"2022-04-14T19:15:02.645891Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef get_f1_of_validation(model):\n\n    imgs = [val_dataset[id][0].unsqueeze(0) for id in range(len(val_dataset))]\n    probs_ims = predict(model, imgs)        # predict - нам написанная функция (она софтмакс к выходам применяет)\n    label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n    y_pred = np.argmax(probs_ims, -1)\n\n    actual_labels = [val_dataset[id][1] for id in range(len(val_dataset))]\n    preds_class = [label_encoder.classes_[i] for i in y_pred]\n\n    return f1_score(actual_labels, y_pred, average='micro')","metadata":{"id":"7BHdstg0mtGT","execution":{"iopub.status.busy":"2022-04-14T19:15:02.647625Z","iopub.execute_input":"2022-04-14T19:15:02.647905Z","iopub.status.idle":"2022-04-14T19:15:02.656434Z","shell.execute_reply.started":"2022-04-14T19:15:02.647833Z","shell.execute_reply":"2022-04-14T19:15:02.655815Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndef make_kaggle_submission(model, test_dataset, output_path='/kaggle/working/my_submission.csv'):\n    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n    probs = predict(model, test_loader)\n\n    preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))   # инверсивно трансформим из цифорок классов в string\n    test_filenames = [path.name for path in test_dataset.files]\n\n    my_submit = pd.DataFrame()\n    my_submit['Id'] = test_filenames\n    my_submit['Expected'] = preds\n    my_submit.to_csv(output_path, index=False)","metadata":{"id":"GtjnqzAUnZuI","execution":{"iopub.status.busy":"2022-04-14T19:15:02.657541Z","iopub.execute_input":"2022-04-14T19:15:02.658089Z","iopub.status.idle":"2022-04-14T19:15:02.668242Z","shell.execute_reply.started":"2022-04-14T19:15:02.658050Z","shell.execute_reply":"2022-04-14T19:15:02.667474Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    '''\n    history: list из tuple вида (train_loss, train_acc, val_loss, val_acc) по эпохам\n    '''\n    loss, acc, val_loss, val_acc = zip(*history)\n    \n    plt.figure(figsize=(12, 7))\n    plt.plot(loss, label=\"train_loss\")\n    plt.plot(val_loss, label=\"val_loss\")\n\n    plt.legend(loc='best')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n\n    plt.show()","metadata":{"id":"3MhqaTXVoNo8","execution":{"iopub.status.busy":"2022-04-14T19:15:02.670435Z","iopub.execute_input":"2022-04-14T19:15:02.670630Z","iopub.status.idle":"2022-04-14T19:15:02.679169Z","shell.execute_reply.started":"2022-04-14T19:15:02.670607Z","shell.execute_reply":"2022-04-14T19:15:02.678298Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"полезная ссылка для fine tune inception v3\nhttps://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705","metadata":{"id":"_qTfb7HX_YPQ"}},{"cell_type":"markdown","source":"---\nснизу пытаюсь разобраться с аугментацией\n---","metadata":{"id":"hylRa-f_2T0f"}},{"cell_type":"markdown","source":"Займусь аугментацией данных, чтобы сделать обучающий датасет побольше, прежде чем обучать какие-то более сложные модели:\n\n- необходимо создать датасет из трансформированных картинок, а потом его конктатенировать с оригинальным датасетом","metadata":{"id":"bhhxEiIea5cr"}},{"cell_type":"code","source":"from torchvision.utils import save_image","metadata":{"id":"A32EwnE-b9rj","execution":{"iopub.status.busy":"2022-04-14T19:15:02.680432Z","iopub.execute_input":"2022-04-14T19:15:02.680943Z","iopub.status.idle":"2022-04-14T19:15:02.687371Z","shell.execute_reply.started":"2022-04-14T19:15:02.680907Z","shell.execute_reply":"2022-04-14T19:15:02.686663Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Код для работы с картинками, чтобы вообще понять: аугментировал я данные или нет","metadata":{"id":"I0WUHws5mWQ4"}},{"cell_type":"code","source":"# Аугментация конечно же применяется только к обучающей выборке:\n\naugm_transf_train = transforms.Compose([                                    \n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomResizedCrop(224 ,scale=(0.5, 1)),  # scale - min; max area of crop\n    transforms.RandomRotation(20),\n    transforms.GaussianBlur(9),     # чем больше ядро - тем суровее блюр очевидно\n    # transforms.RandomPerspective(p=0.5),\n\n    # не стоит пока делать преобразований яркости, поскольку тогда надо будет пересчитывать средние и отклонения по каналам\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),                    \n])","metadata":{"id":"TCZ1ORJ4mem-","execution":{"iopub.status.busy":"2022-04-14T19:15:02.688332Z","iopub.execute_input":"2022-04-14T19:15:02.690203Z","iopub.status.idle":"2022-04-14T19:15:02.698058Z","shell.execute_reply.started":"2022-04-14T19:15:02.690062Z","shell.execute_reply":"2022-04-14T19:15:02.697390Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_augm_dataset = SimpsonsDataset(train_files, mode='train', transform=augm_transf_train)\n\nlen(train_augm_dataset)","metadata":{"id":"c2sszClZkmwn","outputId":"51e5a91b-c0af-4ecc-f3d7-0b6081397a92","execution":{"iopub.status.busy":"2022-04-14T19:15:02.699382Z","iopub.execute_input":"2022-04-14T19:15:02.699880Z","iopub.status.idle":"2022-04-14T19:15:02.864709Z","shell.execute_reply.started":"2022-04-14T19:15:02.699844Z","shell.execute_reply":"2022-04-14T19:15:02.864035Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow для тензоров\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","metadata":{"id":"Z0xjyZ4soCKT","execution":{"iopub.status.busy":"2022-04-14T19:15:02.865943Z","iopub.execute_input":"2022-04-14T19:15:02.866190Z","iopub.status.idle":"2022-04-14T19:15:02.872792Z","shell.execute_reply.started":"2022-04-14T19:15:02.866158Z","shell.execute_reply":"2022-04-14T19:15:02.871960Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"img, label = train_augm_dataset[95]\nname = label_encoder.inverse_transform(np.array([label]))[0]\n\nimshow(img)\nname","metadata":{"id":"i0Jgb7DXohu5","outputId":"fec83868-d836-4301-acef-a23d73975a34","execution":{"iopub.status.busy":"2022-04-14T19:15:02.874403Z","iopub.execute_input":"2022-04-14T19:15:02.874725Z","iopub.status.idle":"2022-04-14T19:15:03.270568Z","shell.execute_reply.started":"2022-04-14T19:15:02.874690Z","shell.execute_reply":"2022-04-14T19:15:03.269870Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Сконкатенируем аугментированные данные и обычные данные:**\n\nДумаю можно особо не париться, что картинки между датасетов объединенных не отшаффлены, поскольку даталоадер уже их отшаффлит, когда мы будем оттуда доставать батчи","metadata":{"id":"hRFDfbrm4y1Q"}},{"cell_type":"code","source":"new_train_dataset = ConcatDataset(\n    [train_dataset, train_augm_dataset]\n)       # ну, по сути это просто \"объединение\" двух классов, здесь нет ничего магического, а то я сначала не особо понял как это возможно","metadata":{"id":"XtDU9Sqb4aa9","execution":{"iopub.status.busy":"2022-04-14T19:15:03.271768Z","iopub.execute_input":"2022-04-14T19:15:03.272389Z","iopub.status.idle":"2022-04-14T19:15:03.276494Z","shell.execute_reply.started":"2022-04-14T19:15:03.272360Z","shell.execute_reply":"2022-04-14T19:15:03.275839Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Fine tuning VGG на новом большом датасете \n\nоб архитектуре VGG:\n\n```python\ndef forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n```\n---\n\n","metadata":{"id":"BsQ9e0mu58ae"}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim import lr_scheduler","metadata":{"id":"Smk3G2Qa6WYu","execution":{"iopub.status.busy":"2022-04-14T19:15:03.278166Z","iopub.execute_input":"2022-04-14T19:15:03.278607Z","iopub.status.idle":"2022-04-14T19:15:03.284997Z","shell.execute_reply.started":"2022-04-14T19:15:03.278569Z","shell.execute_reply":"2022-04-14T19:15:03.284301Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"vgg_model = models.vgg11_bn(pretrained=True)","metadata":{"id":"XR_2Lanq8Spi","outputId":"645a8c80-ade0-46ea-c5a6-6eb6fec479db","execution":{"iopub.status.busy":"2022-04-14T19:15:03.287651Z","iopub.execute_input":"2022-04-14T19:15:03.287841Z","iopub.status.idle":"2022-04-14T19:15:24.705401Z","shell.execute_reply.started":"2022-04-14T19:15:03.287819Z","shell.execute_reply":"2022-04-14T19:15:24.704613Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Что ж, займусь настройкой модели:\n\n(Чтобы заморозить целый блок надо итерироваться по model.modules, например это актуально для inception_v3)","metadata":{"id":"6fqMKeTT7bAM"}},{"cell_type":"code","source":"# average pool перед FC слоями не буду замораживать\nunfreezed_layers = 5\nfor param in vgg_model.features[:-unfreezed_layers]:\n    param.requires_grad = False\n    # print(param.requires_grad)\n\n# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n\nFC_input_size = 25088\nvgg_model.classifier = nn.Sequential(\n            nn.Linear(FC_input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, n_classes),\n        )\n\n# vgg_model.classifier = nn.Linear(FC_input_size, n_classes)","metadata":{"id":"QFcFpv8y7aQ0","execution":{"iopub.status.busy":"2022-04-14T19:15:24.706928Z","iopub.execute_input":"2022-04-14T19:15:24.707205Z","iopub.status.idle":"2022-04-14T19:15:24.762373Z","shell.execute_reply.started":"2022-04-14T19:15:24.707169Z","shell.execute_reply":"2022-04-14T19:15:24.761623Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"vgg_model.to(DEVICE)","metadata":{"id":"jsCYgEWK7_jI","outputId":"473a9772-db26-4782-ea33-c7fb5077a009","execution":{"iopub.status.busy":"2022-04-14T19:15:24.763779Z","iopub.execute_input":"2022-04-14T19:15:24.764113Z","iopub.status.idle":"2022-04-14T19:15:27.543206Z","shell.execute_reply.started":"2022-04-14T19:15:24.764050Z","shell.execute_reply":"2022-04-14T19:15:27.542494Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(vgg_model.parameters(), lr=5e-5, betas=(0.9, 0.999), weight_decay=0.02)\n\n# exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nlmbda = lambda epoch: 0.9 ** epoch\nscheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)","metadata":{"id":"M2FNg3bF6x_3","execution":{"iopub.status.busy":"2022-04-14T19:15:27.544410Z","iopub.execute_input":"2022-04-14T19:15:27.544760Z","iopub.status.idle":"2022-04-14T19:15:27.551432Z","shell.execute_reply.started":"2022-04-14T19:15:27.544725Z","shell.execute_reply":"2022-04-14T19:15:27.550775Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"imshow(new_train_dataset[-100][0])\nlen(new_train_dataset)","metadata":{"id":"gehPOsyRL5Yd","outputId":"a366f92f-1490-40b9-c6c5-d95ec492638a","execution":{"iopub.status.busy":"2022-04-14T19:15:27.554204Z","iopub.execute_input":"2022-04-14T19:15:27.554509Z","iopub.status.idle":"2022-04-14T19:15:27.800119Z","shell.execute_reply.started":"2022-04-14T19:15:27.554474Z","shell.execute_reply":"2022-04-14T19:15:27.799463Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"history = train(train_data=new_train_dataset,\n                val_data=val_dataset,\n                model=vgg_model,\n                epochs=15, batch_size=16,\n                scheduler=scheduler, optimizer=optimizer,\n                save_path='/kaggle/working/vgg.pth')","metadata":{"id":"X-gNqq3APvlt","outputId":"ebc5a003-0b17-4eab-e710-496cdb8fe938","execution":{"iopub.status.busy":"2022-04-14T19:17:15.147933Z","iopub.execute_input":"2022-04-14T19:17:15.148627Z","iopub.status.idle":"2022-04-14T21:32:52.363447Z","shell.execute_reply.started":"2022-04-14T19:17:15.148590Z","shell.execute_reply":"2022-04-14T21:32:52.361786Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"make_kaggle_submission(vgg_model, test_dataset, '/kaggle/working/1vgg_subm.csv')","metadata":{"id":"AbxWBTpTe3ag","execution":{"iopub.status.busy":"2022-04-14T21:33:18.629468Z","iopub.execute_input":"2022-04-14T21:33:18.629731Z","iopub.status.idle":"2022-04-14T21:33:28.807709Z","shell.execute_reply.started":"2022-04-14T21:33:18.629700Z","shell.execute_reply":"2022-04-14T21:33:28.807019Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"- Результат был засабмичен: скор на кэггл: 0.98831\n\n- Сейчас в планах: обучить efficient_net_b2 с batch_size=8 и уже получить желаемую точность в 99.5+","metadata":{}}]}