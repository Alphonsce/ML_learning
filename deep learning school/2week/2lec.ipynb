{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейные модели интерпретирумы, их легко не переобучить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для линейных моделей нужно много предобработки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать сумму квадратов ошибок в качестве Loss - хороший вариант, мы ищем решения в виде линейной функции параметризованной n+1 штук весов,\n",
    "задача - найти эти самые веса путем минимизации $\\sum_{i=1}^l(\\hat{f}(\\vec{x_i})-y_i)^2$\n",
    "\n",
    "$\\vec{x}=(1, x_1, ..., x_n)$\n",
    "\n",
    "$\\hat{f(\\vec{x})}=\\sum_{i=0}^n x_iw_i=\\vec{x}\\cdot\\vec{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия не учитывает взаимодействия между признаками, поэтому иногда может понадобиться создание признаков вида $x_1x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае лин регрессии:\n",
    "\n",
    "$$\n",
    "Xw=y\n",
    "$$\n",
    "\n",
    "Задача-решить СЛУ(приближенно) и найти w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно явно найти выражение для $w$ путем приравнивания производной L к нулю:\n",
    "\n",
    "$$\n",
    "\\vec{w}=(X^TX)^{-1}X^T\\vec{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возникает проблема если примерно есть линейная зависимость колонок X - тогда вычисления станут нестабильными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача классификации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/HV4Bm8UJwIs?t=260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации наша целевая функция - вероятность попадания к классу 1, при условии $\\vec{x}$:\n",
    "\n",
    "$$\n",
    "P(y=1|\\vec{x})=\\hat{f}(\\vec{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы классифицируем на 10 классов, то уже вводится формула Баеса для условной вероятности, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим правдоподобие как произведение вероятностей принадлежности к нужному классу для каждого объекта, потом мы можем прологарифмировать\n",
    "и уже будет сумму логарифмов устремлять к максимуму, а если добавить минус, то:\n",
    "\n",
    "$$\n",
    "-ln(\\prod_{i=1}^l P(y=y_i|\\vec{x_i}))--->min\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда мы берем в качестве функции потерь $ -ln(P(y=y_i|x_i)) $, лосс тогда - минус сумма логарифмов вероятностей предсказания правильного класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем искать нашу целевую функцию в виде:\n",
    "$$\n",
    "\\sigma(\\sum_{i=0}^n x_iw_i)\n",
    "$$\n",
    "\n",
    "Будем интерпретировать значение этой функии как вероятность принадлежности к 1 классу (если бинарная классификация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обобщение на много классов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/twdZBko9vH4?t=423\n",
    "\n",
    "Softmax в качестве выхода дает вектор из вероятностей принадлежности к каждому классу, принимая вектор из логитов, \n",
    "\n",
    "вектор из логитов мы получаем, получая для каждого класса свои веса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax - это по-сути и есть типо мягкий максимум, он делает самое большое число явно больше всех остальных, но остальные не зануляет и оставляет\n",
    "функцию дифференцируемой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как выглядит в итоге вероятность принадлежности объекта к k-му классу: https://youtu.be/twdZBko9vH4?t=722\n",
    "\n",
    "$$\n",
    "P(y=k| \\vec{x}) = \\dfrac{logits(\\vec{x})_k}{\\sum_{i=1}^m logits(\\vec{x})_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим прийти в значение локального минимума функции f:\n",
    "$$\n",
    "\\vec{x}=\\vec{x}-\\alpha\\nabla f(\\vec{x})\n",
    "$$\n",
    "(значение градиента считаем в текущей точке и затем получаем новое значение x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация для линейных моделей.\n",
    "\n",
    "Если есть линейная зависимость между колонками, то у нас бесконечно много весов одинаково оптимизируют лосс, а работать с очень большими по модулю\n",
    "весами плохо - все вычисления нестабильные, поэтому надо в лосс добавлять $+\\beta\\sum_{j=1}w_j^2$ - тогда мы будет балансировать между величиной\n",
    "значения весов и минимазиацией эмпирического риска.\n",
    "\n",
    "То есть к частной производной лосса по $w_j$ добавиться $2\\beta\\sum w_j$, что будет немного регулировать значения весов и немного их уменьшать с каждым шагом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Лучше работает град спуск\n",
    "- Регуляризация лучше работает\n",
    "- Для нормированных данных все говорят о важности признаков, посольку все признаки нормализованы и по порядку они уже не отличаются"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
