{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR problem:\n",
    "\n",
    "- однослойным перцептроном (это по сути просто логрег на несколько классов) не реализовать XOR, но если добавить еще один признак - произведение признаков, то получится\n",
    "- Или xor реализется использованием 2 слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерес к нейронкам угас в нулевых, потому что был придуман градиентный бустинг, который почти не переобучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовый логрег"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О логистической регрессии (для многоклассовой классификации на m классов): \n",
    "- Перед выходом в нейронной сети применяется софтмакс для получения вектора вероятностей, а из него уже можно класс выбирать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Мы хотим, чтобы logreg возвращал нам вектор из $P(y=k|x)$ - вероятностей того, что наш объект принадлежит к k классу, это мы будем делать, положив предсказательную функцию (или функцию активации, если рассматривать логрег, как однослойную нейронку) в виде\n",
    "$$\n",
    "softmax( logits(\\vec{x}) ) - \\text{вектор из вероятностей}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- По сути логрег строит гиперплоскости, разделяющие всю выборку, а вероятность принадлежности к классу - тоже связана с расстояниями до каждой плоскости:\n",
    "$$\n",
    "P(y=k| \\vec{x}) = \\dfrac{exp(logits(\\vec{x})_k)}{\\sum_{i=1}^m exp(logits(\\vec{x})_i)} \\text{( - k-ая компонента softmax(logits(x)))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора весов в логитах используется лосс - logloss - это минус логарифм правдоподобия (у нас есть вектор весов + свободный член для каждого класса, в случае бинарной классификации нам хвтает лишь одного вектора весов+свобдного члена, потому что P(0|x)=1-P(1|x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в обычном логреге мы просто градиентным спуском находим минимум logloss и каждый шаг обновляем все вектора весов + своб чл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- С точки зрения слоя нейронной сети:\n",
    "\n",
    "W - это матрица из векторов весов для каждого класса, b - вектор из св чл для каждого класса, то есть на втором этапе мы получаем вектор из логитов, XW - вектор из вероятностей, его длина - количество классов. (X - строка)\n",
    "\n",
    "Пусть X имеет 30 признаков, а классов у нас 50, тогда: у W столбцы - это вектора весов для каждого класса, то есть W: 30x50\n",
    "\n",
    "$$\n",
    "XW=(\\vec{w}_1\\cdot X,...,\\vec{w}_m\\cdot X)^T, \\text{ здесь } \\vec{w}_k \\text{- это целый вектор весов для k класса}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{X}---> XW + b ---> (P(1|X),...,P(k|X))^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Про нейронки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- А если вдруг выборка линейно не разделима? (можно использовать SVM и подобрать в нем ядро или ансамбли деревьев)\n",
    "\n",
    "- SVM - это задача поиска разделяющей плоскости для двух классов так, чтобы было максимальным самое маленькое расстояние, то есть нам надо максимизировать $1/||w||$, если выборка линейно разделима, то тут все очевидно, а если нет, то надо подбирать ядра, то есть надо добавить какой-то новый столбец признаков так, чтобы выборка стала разделимой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Нейронные сети теперь будут моделировать фичи за нас и только потом классифицировать, что имеется в виду:\n",
    "\n",
    "- Мы к исходной выборке применяем линейную модель и потом к результату этой модели применяем функцию активации, таким образом нейронная сеть за нас подбирает что-то наподобие kernel для SVM, схемка:\n",
    "\n",
    "x $\\in X$ -->(Линейн преобр) x' $\\in X'$ (у X' уже другая размерность) -->(применяем нелинейную ф активации) -->(Logreg) --> Output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Пример:\n",
    "\n",
    "1) На вход подаем X - пространство признаков размерности 10, затем при помощи первой линейной модели переводим в $XW_1$ - получаем, например, пространство признаков размерности 50, \n",
    "   \n",
    "2) Потом применяем функцию активации с каждому полученному признаку, т.к. иначе во всем этом нет смысла и мы просто строим одну линейную модель, потому что сумма линейных комбинаций - линейная комбинация, а так функции активации позволяют нам работать с любой выборкой, даже линейно неразделимой, то есть линейная модель + функция активации - это что-то типо настроемого ядра SVM\n",
    "\n",
    "(Если не применять функции активации, то каскад линейных преобразований схлопнется просто в одно линейное преобразование):\n",
    "$$\n",
    "(XW + b)\\hat{W} + \\hat{b}=XW\\hat{W} +b\\hat{W}+\\hat{b}\n",
    "$$\n",
    "   \n",
    "3) И теперь мы имеем некоторое пространство размерности 50 с новыми признаками - для него теперь мы применяем логистическую регрессию: получаем вектор вероятностей принадлежности к каждому классу\n",
    "\n",
    "4) Ну и теперь уже делаем как хотим - либо output - это наиболее вероятный класс либо просто весь вектор вероятностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь решающий алгоритм имеет вид длинной цепочки композиции линейных моделей и функций активации, которая завершается финальным логрегом, конечно же, наши градиентные методы оптимизации до сих пор работают и мы до сих пор можем оптимизировать наш Logloss для последней логрегрессии, но для этого надо побольше проивзодных посчитать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сила нейронных сетей в дифференцируемости всего (главное доопределить так функции чтобы они 1 раз были диф-мы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции активации: Они нужны для того, чтобы сделать некоторые нелинейности\n",
    "\n",
    "1)сигмоида\n",
    "\n",
    "2)tanhx\n",
    "\n",
    "3)max(0, x) - ReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Понятия:\n",
    "\n",
    "Layer - структурная единица в цепочке композиции функций, которые применяются ко входному объекту (линейный слой можно мыслить как афинное преобразование в афинном пространстве признаков) \n",
    "\n",
    "Функция активации - то, что всегда применяется к результату линейого слоя (афинного преобразования) перед ним \n",
    "\n",
    "Backpropagation - \"fancy word for chain rule\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея в том, что нам все так же надо оптимизировать функцию потерь, однако теперь она явно не зависит от весов, которые мы подбираем для минимизации лосса.\n",
    "\n",
    "Но функция потерь теперь зависит от значения функции активации, которая зависит от весов. Используя это нам надо получить производную лосса по весу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- См pdf для подробных объяснений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backprop сразу на всех слоях позволяет менять веса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Про функции активации подробнее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронная сеть с каждым шагом меняет признаковое представление объекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Сигмоида\n",
    "+ имеет конечкую область значений\n",
    "+ неплохо интерпретируется\n",
    "  \n",
    "- Затухает на хвостах: довольно большие хвосты с околонулевым градиентом из-за чего модель на этих ховстах не будет практически обновлять веса\n",
    "- нецентрированный выход => сдвиг постоянный\n",
    "- cчитать сложно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) tanh\n",
    "+ как сигмоида, но еще и центрирована"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) ReLU\n",
    "- легко считать производную\n",
    "- градиент не затухает\n",
    "- выход не центрирован\n",
    "- умирает при отрицательном входе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Leaky ReLU = max(0.01x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Функции активации подбирают под задачу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если не знать какую юзать, то:\n",
    "- Использовать ReLU\n",
    "- не использовать sigmoid\n",
    "- попробовать Leaky ReLU/ELU, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cамая простая версия - это fully connected - это просто нейронка из линейных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) RNN (recurent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они умеют генерировать код/текст и тп по скормленным ей примерам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похожи на то, как работает зрительная кора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нейронной сети (пока) все должно быть дифференцируемо, поэтому не стоит линейный переход заменять на решающее дерево, иначе мы просто не сможем бэкпропнуть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подробно про полносвязную нейронную сеть:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель нейрона:\n",
    "\n",
    "Значение внутри нейрона это по-сути просто один из новых признаков в текущем слое, в нем лежит взвешенная суммы предыдущих признаков с весами, соответствующими этому нейрону, а на выход он передает функцию активации от этой взвешенной суммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Многослойный перцептрон:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Вся сеть состоит только из fully connected слоев, суть заключается просто в том, что мы на скрытых слоях постоянно переходим к новому признаковому описанию нашей выборки, а потом в конце делаем логистическую регрессию на выходе последнего скрытого слоя (или если задача классификации, то другое что-то делаем)\n",
    "  \n",
    "- полученный вектор softmax и будет вероятностями принадлежности к каждому классу\n",
    "\n",
    "- Или можно мыслить это эквивалентно так: все скрытые слои - это переходы к новым признакам при помощи функции активации на взвешенной сумме, а для выходного слоя мы применяем softamx в качестве функции активации на предпоследнем слое(так как внутри нейроной у нас лежат взвешенные суммы, а они же и есть логиты для данного нейрона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Если задачу регрессии решаем, то просто в последнем слое решаем задачу регрессии на полученной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Но надо понимать, что мы не можем просто взять и перед выходным слоем заюзать какой-то недифференцируемый алгоритм, поскольку тогда мы просто не сможем посчитать производную и обновить веса, поэтому все устроенно так, как устроенно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции записывают объект в столбец признаков, а весы нейрона в строчку, но мне вообще удобнее наоборот, но из контекста будет очевидно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: подобрать матрицы весов и вектора сдвигов так, чтобы минимизировать лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- для классификации в качестве лосса просто logloss юзается обычно\n",
    "- MSE для регресси можно юзать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/O0nGKKFyYT4?t=1705 - для backprop"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8b603c973ef7f83aae64b632e2e67529bc0c014d258c607b969039a8c89a028"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
