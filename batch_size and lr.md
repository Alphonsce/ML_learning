## Некоторые заметки по поводу размера батча, количества эпох и learning rate:
---

Мы обновляем градиент каждый раз, когда проходимся по батчу, поскольку датасеты огромных размеров. 

- Батч - случайная подвыборка нашего датасета.

- Эпоха - один проход по всем элементам выборки (обновление градиента по каждому батчу - 1 эпоха)

**Если мы фиксируем количество эпох, то:**

Learning rate и batch size регулируют нашу итоговую сходимость и время этой сходимости про окончании всех эпох. При фиксированном количестве эпох:
 
- чем меньше batch_size - тем больше раз мы обновим градиент, но, чем меньше batch_size - тем более случайное движение в сторону минимума - мы можем не попасть в наиболее оптимальную точку, но мы больше раз сделаем шаг в сторону минимума L.

- Если же batch_size больше, то мы сделаем меньше шагов в сторону минимума градиента, но они будут направлены менее случайно относительно экстремальной точки.

Получается, что большой batch_size дает нам более долгую и более точную сходимость к экстремальной точке, а маленький batch_size дает нам более быструю сходимость, но к некоторой условно оптимальной точке. (Если количество эпох фиксировано).

**Пример:** 

Если у нас dataset из 1000 элементов и мы хотим сделать 10 эпох обучения при одинаковом learning_rate:

Если мы выбираем batch_size = 100, то каждая эпоха - это 10 итераций, то есть все обучение займет 100 итераций. Здесь мы будем более плавно и медленне спускаться к минимуму L.

Если batch_size = 10, то кажда эпоха - 100 итераций, то есть теперь все обучение будет занимать 1000 итераций. Здесь зависимость L от эпохи может быть довольно-таки пилообразной, потому что случайность шага немного выше, однако мы за 1 эпоху делаем больше шагов.


Чтобы как-то уровнять эти два случая можно повысить learning rate для батчей большего размера, тогда мы будет делать более точные, но более длинные шаги, что в принципе тоже не особо хорошо, но у нас нет выбора.